{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documenta\u00e7\u00e3o do projeto Este projeto \u00e9 um exemplo de um sistema de Ecommerce focado para ser generalista e atender a maioria dos casos de uso comuns em sistemas de lojas virtuais. O intuito \u00e9 gerar insights valiosos por meio da visualiza\u00e7\u00e3o de dados, utilizando de t\u00e9cnicas de pipelines de dados para a ingest\u00e3o, transforma\u00e7\u00e3o e visualiza\u00e7\u00e3o de dados. Visite as abas ao lado para mais informa\u00e7\u00f5es sobre o projeto.","title":"Home"},{"location":"#documentacao-do-projeto","text":"Este projeto \u00e9 um exemplo de um sistema de Ecommerce focado para ser generalista e atender a maioria dos casos de uso comuns em sistemas de lojas virtuais. O intuito \u00e9 gerar insights valiosos por meio da visualiza\u00e7\u00e3o de dados, utilizando de t\u00e9cnicas de pipelines de dados para a ingest\u00e3o, transforma\u00e7\u00e3o e visualiza\u00e7\u00e3o de dados. Visite as abas ao lado para mais informa\u00e7\u00f5es sobre o projeto.","title":"Documenta\u00e7\u00e3o do projeto"},{"location":"arquitetura/","text":"Arquitetura do Projeto Banco relacional A ingest\u00e3o de dados \u00e9 feita a partir de um banco de dados relacional, onde s\u00e3o armazenados os dados brutos do sistema de E-commerce. ETL Fluxo da pipeline explicado em ETL Ferramentas Para saber mais sobre as ferramentas utilizadas no projeto, acesse Ferramentas Banco dimensional O banco dimensional \u00e9 o respons\u00e1vel por armazenar os dados de forma otimizada para a realiza\u00e7\u00e3o de consultas anal\u00edticas. O mesmo possui as seguintes tabelas: Dimens\u00e3o de Categorias de Produtos Dimens\u00e3o de Localiza\u00e7\u00e3o Dimens\u00e3o de Vendedores Dimens\u00e3o de Clientes Fato de Vendas","title":"Arquitetura"},{"location":"arquitetura/#arquitetura-do-projeto","text":"","title":"Arquitetura do Projeto"},{"location":"arquitetura/#banco-relacional","text":"A ingest\u00e3o de dados \u00e9 feita a partir de um banco de dados relacional, onde s\u00e3o armazenados os dados brutos do sistema de E-commerce.","title":"Banco relacional"},{"location":"arquitetura/#etl","text":"Fluxo da pipeline explicado em ETL","title":"ETL"},{"location":"arquitetura/#ferramentas","text":"Para saber mais sobre as ferramentas utilizadas no projeto, acesse Ferramentas","title":"Ferramentas"},{"location":"arquitetura/#banco-dimensional","text":"O banco dimensional \u00e9 o respons\u00e1vel por armazenar os dados de forma otimizada para a realiza\u00e7\u00e3o de consultas anal\u00edticas. O mesmo possui as seguintes tabelas: Dimens\u00e3o de Categorias de Produtos Dimens\u00e3o de Localiza\u00e7\u00e3o Dimens\u00e3o de Vendedores Dimens\u00e3o de Clientes Fato de Vendas","title":"Banco dimensional"},{"location":"dashboard/","text":"Dashboard Introdu\u00e7\u00e3o Este documento descreve o processo de cria\u00e7\u00e3o de um dashboard utilizando o Power BI, a partir de dados recebidos por meio de uma pipeline de dados. Al\u00e9m disso aborda a utiliza\u00e7\u00e3o de recursos adicionais, como v\u00eddeos no YouTube e artigos, para garantir a entrega de um dashboard de alta qualidade. Ferramenta Utilizamos o Power BI por ser uma ferramenta de visualiza\u00e7\u00e3o de dados da Microsoft que permite a cria\u00e7\u00e3o de dashboards interativos e informativos. Com uma interface intuitiva e diversas op\u00e7\u00f5es de visualiza\u00e7\u00e3o, o Power BI \u00e9 amplamente utilizado em empresas de diversos setores para an\u00e1lise de dados e tomada de decis\u00f5es. Passos executados 1. Coleta e Pr\u00e9-Processamento de Dados Defini\u00e7\u00e3o dos Requisitos de Dados Identifica\u00e7\u00e3o das fontes de dados necess\u00e1rias. Determina\u00e7\u00e3o dos KPIs e m\u00e9tricas relevantes para o dashboard. Configura\u00e7\u00e3o da Pipeline de Dados 2. Importa\u00e7\u00e3o de Dados para o Power BI Conex\u00e3o \u00e0s Fontes de Dados Utiliza\u00e7\u00e3o das funcionalidades de importa\u00e7\u00e3o de dados do Power BI. Configura\u00e7\u00e3o de conex\u00f5es a bancos de dados, arquivos CSV, Excel, APIs, etc. Modelagem de Dados Cria\u00e7\u00e3o de relacionamentos entre tabelas. 3. Cria\u00e7\u00e3o do Dashboard Design e Layout do Dashboard Escolha de visualiza\u00e7\u00f5es apropriadas (gr\u00e1ficos de barras, gr\u00e1ficos de linhas, etc.). Inser\u00e7\u00e3o e configura\u00e7\u00e3o de gr\u00e1ficos e tabelas no Power BI. Personaliza\u00e7\u00e3o de cores, fontes e outros aspectos visuais para melhorar a apresenta\u00e7\u00e3o. Adi\u00e7\u00e3o de filtros para permitir a explora\u00e7\u00e3o interativa dos dados. Configura\u00e7\u00e3o de drill-throughs e links para aprofundar em detalhes espec\u00edficos. 4. Valida\u00e7\u00e3o e Refinamento Verifica\u00e7\u00e3o de Consist\u00eancia dos Dados Checagem de precis\u00e3o e integridade dos dados importados e modelados. Compara\u00e7\u00e3o com fontes originais para garantir consist\u00eancia. Testes de Usabilidade. Refer\u00eancias V\u00eddeos no Youtube: \"Power BI Full Course - Learn Power BI in 4 Hours | Power BI Tutorial | Edureka\" : Tutorial abrangente que cobre desde a importa\u00e7\u00e3o de dados at\u00e9 a cria\u00e7\u00e3o de dashboards. \"Advanced Power BI Techniques for Building Effective Dashboards | Guy in a Cube\" : T\u00e9cnicas avan\u00e7adas para aprimorar a funcionalidade e a est\u00e9tica dos dashboards. Artigos e Blogs: \"10 Tips for Creating Effective Power BI Dashboards\" (Microsoft Power BI Blog) : Dicas pr\u00e1ticas para criar dashboards eficientes e informativos. \"Data Visualization Best Practices in Power BI\" (SQLBI) : Melhores pr\u00e1ticas para visualiza\u00e7\u00e3o de dados utilizando Power BI.","title":"Dashboard"},{"location":"dashboard/#dashboard","text":"","title":"Dashboard"},{"location":"dashboard/#introducao","text":"Este documento descreve o processo de cria\u00e7\u00e3o de um dashboard utilizando o Power BI, a partir de dados recebidos por meio de uma pipeline de dados. Al\u00e9m disso aborda a utiliza\u00e7\u00e3o de recursos adicionais, como v\u00eddeos no YouTube e artigos, para garantir a entrega de um dashboard de alta qualidade.","title":"Introdu\u00e7\u00e3o"},{"location":"dashboard/#ferramenta","text":"Utilizamos o Power BI por ser uma ferramenta de visualiza\u00e7\u00e3o de dados da Microsoft que permite a cria\u00e7\u00e3o de dashboards interativos e informativos. Com uma interface intuitiva e diversas op\u00e7\u00f5es de visualiza\u00e7\u00e3o, o Power BI \u00e9 amplamente utilizado em empresas de diversos setores para an\u00e1lise de dados e tomada de decis\u00f5es.","title":"Ferramenta"},{"location":"dashboard/#passos-executados","text":"1. Coleta e Pr\u00e9-Processamento de Dados Defini\u00e7\u00e3o dos Requisitos de Dados Identifica\u00e7\u00e3o das fontes de dados necess\u00e1rias. Determina\u00e7\u00e3o dos KPIs e m\u00e9tricas relevantes para o dashboard. Configura\u00e7\u00e3o da Pipeline de Dados 2. Importa\u00e7\u00e3o de Dados para o Power BI Conex\u00e3o \u00e0s Fontes de Dados Utiliza\u00e7\u00e3o das funcionalidades de importa\u00e7\u00e3o de dados do Power BI. Configura\u00e7\u00e3o de conex\u00f5es a bancos de dados, arquivos CSV, Excel, APIs, etc. Modelagem de Dados Cria\u00e7\u00e3o de relacionamentos entre tabelas. 3. Cria\u00e7\u00e3o do Dashboard Design e Layout do Dashboard Escolha de visualiza\u00e7\u00f5es apropriadas (gr\u00e1ficos de barras, gr\u00e1ficos de linhas, etc.). Inser\u00e7\u00e3o e configura\u00e7\u00e3o de gr\u00e1ficos e tabelas no Power BI. Personaliza\u00e7\u00e3o de cores, fontes e outros aspectos visuais para melhorar a apresenta\u00e7\u00e3o. Adi\u00e7\u00e3o de filtros para permitir a explora\u00e7\u00e3o interativa dos dados. Configura\u00e7\u00e3o de drill-throughs e links para aprofundar em detalhes espec\u00edficos. 4. Valida\u00e7\u00e3o e Refinamento Verifica\u00e7\u00e3o de Consist\u00eancia dos Dados Checagem de precis\u00e3o e integridade dos dados importados e modelados. Compara\u00e7\u00e3o com fontes originais para garantir consist\u00eancia. Testes de Usabilidade.","title":"Passos executados"},{"location":"dashboard/#referencias","text":"V\u00eddeos no Youtube: \"Power BI Full Course - Learn Power BI in 4 Hours | Power BI Tutorial | Edureka\" : Tutorial abrangente que cobre desde a importa\u00e7\u00e3o de dados at\u00e9 a cria\u00e7\u00e3o de dashboards. \"Advanced Power BI Techniques for Building Effective Dashboards | Guy in a Cube\" : T\u00e9cnicas avan\u00e7adas para aprimorar a funcionalidade e a est\u00e9tica dos dashboards. Artigos e Blogs: \"10 Tips for Creating Effective Power BI Dashboards\" (Microsoft Power BI Blog) : Dicas pr\u00e1ticas para criar dashboards eficientes e informativos. \"Data Visualization Best Practices in Power BI\" (SQLBI) : Melhores pr\u00e1ticas para visualiza\u00e7\u00e3o de dados utilizando Power BI.","title":"Refer\u00eancias"},{"location":"etl/","text":"ETL Introdu\u00e7\u00e3o O Processo de ETL consiste em enviar dados de uma base para outra (ou para um data lake) da forma mais otimizada poss\u00edvel e realizando as devidas formata\u00e7\u00f5es e transforma\u00e7\u00f5es. Ferramentas Apache Airflow - Ferramenta de orquestra\u00e7\u00e3o de pipelines Apache Spark - Ferramenta de processamento de dados PySpark - Biblioteca Python para processamento de dados MiniO - Object Storage PostgreSql - Banco de dados relacional Passos Executados Atrav\u00e9s do Apache Airflow n\u00f3s realizamos a chamada de uma Dag (processo de pipeline que neste caso ser\u00e1 utilizado para realizar o envio de dados de uma base de dados de Ecommerce para uma base de dados dimensional), possibilitando assim, melhor an\u00e1lise dos dados. A Dag ETL (Dag configurada no Airflow para realizar esta opera\u00e7\u00e3o) realizar\u00e1 a chamada do processo etl_get_data que tem como objetivo buscar os dados do banco PostgreSql e criar arquivos .csv contendo 1000 dados cada, facilitando assim a leitura destes. Ap\u00f3s o processo etl_get_data ser concluido, realizamos a chamada do processo etl_leading , que tem como objetivo ler os arquivos .csv contendo os dados do banco e enviar eles para o MiniO que neste contexto servir\u00e1 como Object Storage. Estes arquivos s\u00e3o salvos no MiniO no formato parquet . Sendo concluido o processo da camada leading, o Airflow realiza a chamada do processo etl_bronze que tem como objetivo ler os arquivos parquet e criar uma Delta Table destes, otimizando assim as consultas subsequentes. Posteriormente, ser\u00e1 chamado o processo etl_silver que ser\u00e1 respons\u00e1vel pela transforma\u00e7\u00e3o dos dados nos padr\u00f5es necess\u00e1rios \u00daltimo processo dentro da arquitetura medalh\u00e3o, o etl_gold , que enviar\u00e1 os dados que est\u00e3o nas tabelas delta para o banco dimensional PostgreSql .","title":"ETL"},{"location":"etl/#etl","text":"","title":"ETL"},{"location":"etl/#introducao","text":"O Processo de ETL consiste em enviar dados de uma base para outra (ou para um data lake) da forma mais otimizada poss\u00edvel e realizando as devidas formata\u00e7\u00f5es e transforma\u00e7\u00f5es.","title":"Introdu\u00e7\u00e3o"},{"location":"etl/#ferramentas","text":"Apache Airflow - Ferramenta de orquestra\u00e7\u00e3o de pipelines Apache Spark - Ferramenta de processamento de dados PySpark - Biblioteca Python para processamento de dados MiniO - Object Storage PostgreSql - Banco de dados relacional","title":"Ferramentas"},{"location":"etl/#passos-executados","text":"Atrav\u00e9s do Apache Airflow n\u00f3s realizamos a chamada de uma Dag (processo de pipeline que neste caso ser\u00e1 utilizado para realizar o envio de dados de uma base de dados de Ecommerce para uma base de dados dimensional), possibilitando assim, melhor an\u00e1lise dos dados. A Dag ETL (Dag configurada no Airflow para realizar esta opera\u00e7\u00e3o) realizar\u00e1 a chamada do processo etl_get_data que tem como objetivo buscar os dados do banco PostgreSql e criar arquivos .csv contendo 1000 dados cada, facilitando assim a leitura destes. Ap\u00f3s o processo etl_get_data ser concluido, realizamos a chamada do processo etl_leading , que tem como objetivo ler os arquivos .csv contendo os dados do banco e enviar eles para o MiniO que neste contexto servir\u00e1 como Object Storage. Estes arquivos s\u00e3o salvos no MiniO no formato parquet . Sendo concluido o processo da camada leading, o Airflow realiza a chamada do processo etl_bronze que tem como objetivo ler os arquivos parquet e criar uma Delta Table destes, otimizando assim as consultas subsequentes. Posteriormente, ser\u00e1 chamado o processo etl_silver que ser\u00e1 respons\u00e1vel pela transforma\u00e7\u00e3o dos dados nos padr\u00f5es necess\u00e1rios \u00daltimo processo dentro da arquitetura medalh\u00e3o, o etl_gold , que enviar\u00e1 os dados que est\u00e3o nas tabelas delta para o banco dimensional PostgreSql .","title":"Passos Executados"},{"location":"ferramentas/","text":"Ferramentas Introdu\u00e7\u00e3o Este documento descreve as ferramentas utilizadas no projeto, incluindo as tecnologias de banco de dados, linguagens de programa\u00e7\u00e3o, ferramentas de orquestra\u00e7\u00e3o de pipelines e armazenamento de objetos. Utilizamos as seguintes ferramentas: PostgreSQL - Banco de dados relacional Python - Linguagem de programa\u00e7\u00e3o utilizada para criar os scripts de extra\u00e7\u00e3o, transforma\u00e7\u00e3o e carga de dados Docker - Containeriza\u00e7\u00e3o de aplica\u00e7\u00f5es Apache Spark - Processamento de dados em larga escala Apache Airflow - Orquestrador de tarefas Visual Studio Code - Editor de c\u00f3digo Astro CLI - Ferramenta de linha de comando para gerenciamento de infraestrutura Minio - Armazenamento de objetos Descri\u00e7\u00e3o PostgreSQL O PostgreSQL \u00e9 um sistema de gerenciamento de banco de dados relacional de c\u00f3digo aberto e amplamente utilizado. Ele oferece suporte a recursos avan\u00e7ados de SQL e \u00e9 conhecido por sua confiabilidade, escalabilidade e extensibilidade. Python Python \u00e9 uma linguagem de programa\u00e7\u00e3o de alto n\u00edvel, interpretada e de prop\u00f3sito geral. \u00c9 amplamente utilizada em ci\u00eancia de dados, desenvolvimento web, automa\u00e7\u00e3o de tarefas e muito mais. No projeto de Ecommerce, utilizamos Python para criar scripts de ETL e outras tarefas de processamento de dados. Docker Docker \u00e9 uma plataforma de c\u00f3digo aberto que facilita a cria\u00e7\u00e3o, implanta\u00e7\u00e3o e execu\u00e7\u00e3o de aplicativos em cont\u00eaineres. Ele permite que os desenvolvedores empacotem uma aplica\u00e7\u00e3o com todas as suas depend\u00eancias em um cont\u00eainer, garantindo que ela funcione de forma consistente em qualquer ambiente. Apache Spark Apache Spark \u00e9 um framework de processamento de dados em larga escala, conhecido por sua velocidade e facilidade de uso. Ele oferece suporte a v\u00e1rias linguagens de programa\u00e7\u00e3o, incluindo Python, e \u00e9 amplamente utilizado para an\u00e1lise de dados, machine learning e processamento de streaming. Apache Airflow Apache Airflow \u00e9 uma plataforma de orquestra\u00e7\u00e3o de pipelines de dados, que permite agendar, monitorar e executar tarefas de forma automatizada. Ele oferece suporte a fluxos de trabalho complexos e \u00e9 altamente configur\u00e1vel, tornando-o uma escolha popular para a automa\u00e7\u00e3o de processos de ETL e an\u00e1lise de dados. Visual Studio Code Visual Studio Code \u00e9 um editor de c\u00f3digo leve, poderoso e altamente personaliz\u00e1vel, desenvolvido pela Microsoft. Ele oferece suporte a v\u00e1rias linguagens de programa\u00e7\u00e3o e possui uma ampla variedade de extens\u00f5es que facilitam o desenvolvimento de aplicativos e scripts. Astro CLI Astro CLI \u00e9 uma ferramenta de linha de comando para gerenciamento de infraestrutura, desenvolvida pela Astronomer. Ela simplifica a implanta\u00e7\u00e3o e o gerenciamento de pipelines de dados baseados em Apache Airflow, permitindo que os usu\u00e1rios criem, execute e monitorem tarefas de forma eficiente. Minio Minio \u00e9 um servi\u00e7o de armazenamento de objetos de c\u00f3digo aberto, compat\u00edvel com o Amazon S3. Ele oferece escalabilidade, alta disponibilidade e seguran\u00e7a para o armazenamento de dados n\u00e3o estruturados, como imagens, v\u00eddeos e arquivos de log. Instala\u00e7\u00e3o Para instalar as ferramentas utilizadas no projeto, basta ir na raiz do projeto e executar o comando: docker compose up","title":"Ferramentas"},{"location":"ferramentas/#ferramentas","text":"","title":"Ferramentas"},{"location":"ferramentas/#introducao","text":"Este documento descreve as ferramentas utilizadas no projeto, incluindo as tecnologias de banco de dados, linguagens de programa\u00e7\u00e3o, ferramentas de orquestra\u00e7\u00e3o de pipelines e armazenamento de objetos. Utilizamos as seguintes ferramentas: PostgreSQL - Banco de dados relacional Python - Linguagem de programa\u00e7\u00e3o utilizada para criar os scripts de extra\u00e7\u00e3o, transforma\u00e7\u00e3o e carga de dados Docker - Containeriza\u00e7\u00e3o de aplica\u00e7\u00f5es Apache Spark - Processamento de dados em larga escala Apache Airflow - Orquestrador de tarefas Visual Studio Code - Editor de c\u00f3digo Astro CLI - Ferramenta de linha de comando para gerenciamento de infraestrutura Minio - Armazenamento de objetos","title":"Introdu\u00e7\u00e3o"},{"location":"ferramentas/#descricao","text":"PostgreSQL O PostgreSQL \u00e9 um sistema de gerenciamento de banco de dados relacional de c\u00f3digo aberto e amplamente utilizado. Ele oferece suporte a recursos avan\u00e7ados de SQL e \u00e9 conhecido por sua confiabilidade, escalabilidade e extensibilidade. Python Python \u00e9 uma linguagem de programa\u00e7\u00e3o de alto n\u00edvel, interpretada e de prop\u00f3sito geral. \u00c9 amplamente utilizada em ci\u00eancia de dados, desenvolvimento web, automa\u00e7\u00e3o de tarefas e muito mais. No projeto de Ecommerce, utilizamos Python para criar scripts de ETL e outras tarefas de processamento de dados. Docker Docker \u00e9 uma plataforma de c\u00f3digo aberto que facilita a cria\u00e7\u00e3o, implanta\u00e7\u00e3o e execu\u00e7\u00e3o de aplicativos em cont\u00eaineres. Ele permite que os desenvolvedores empacotem uma aplica\u00e7\u00e3o com todas as suas depend\u00eancias em um cont\u00eainer, garantindo que ela funcione de forma consistente em qualquer ambiente. Apache Spark Apache Spark \u00e9 um framework de processamento de dados em larga escala, conhecido por sua velocidade e facilidade de uso. Ele oferece suporte a v\u00e1rias linguagens de programa\u00e7\u00e3o, incluindo Python, e \u00e9 amplamente utilizado para an\u00e1lise de dados, machine learning e processamento de streaming. Apache Airflow Apache Airflow \u00e9 uma plataforma de orquestra\u00e7\u00e3o de pipelines de dados, que permite agendar, monitorar e executar tarefas de forma automatizada. Ele oferece suporte a fluxos de trabalho complexos e \u00e9 altamente configur\u00e1vel, tornando-o uma escolha popular para a automa\u00e7\u00e3o de processos de ETL e an\u00e1lise de dados. Visual Studio Code Visual Studio Code \u00e9 um editor de c\u00f3digo leve, poderoso e altamente personaliz\u00e1vel, desenvolvido pela Microsoft. Ele oferece suporte a v\u00e1rias linguagens de programa\u00e7\u00e3o e possui uma ampla variedade de extens\u00f5es que facilitam o desenvolvimento de aplicativos e scripts. Astro CLI Astro CLI \u00e9 uma ferramenta de linha de comando para gerenciamento de infraestrutura, desenvolvida pela Astronomer. Ela simplifica a implanta\u00e7\u00e3o e o gerenciamento de pipelines de dados baseados em Apache Airflow, permitindo que os usu\u00e1rios criem, execute e monitorem tarefas de forma eficiente. Minio Minio \u00e9 um servi\u00e7o de armazenamento de objetos de c\u00f3digo aberto, compat\u00edvel com o Amazon S3. Ele oferece escalabilidade, alta disponibilidade e seguran\u00e7a para o armazenamento de dados n\u00e3o estruturados, como imagens, v\u00eddeos e arquivos de log.","title":"Descri\u00e7\u00e3o"},{"location":"ferramentas/#instalacao","text":"Para instalar as ferramentas utilizadas no projeto, basta ir na raiz do projeto e executar o comando: docker compose up","title":"Instala\u00e7\u00e3o"},{"location":"padroes/","text":"Padr\u00f5es do projeto Documenta\u00e7\u00e3o Focamos em criar uma documenta\u00e7\u00e3o concisa, utilizando os melhores padr\u00f5es de escrita e organiza\u00e7\u00e3o de documentos. Para isso, utilizamos o Markdown, que \u00e9 uma linguagem de marca\u00e7\u00e3o leve e f\u00e1cil de usar, respons\u00e1vel tanto pelas documenta\u00e7\u00f5es que voc\u00ea est\u00e1 lendo e as que est\u00e3o dispon\u00edveis no reposit\u00f3rio. Os arquivos recomendados para a documenta\u00e7\u00e3o s\u00e3o: README.md - Documenta\u00e7\u00e3o principal do projeto CHANGELOG.md - Registro de todas as altera\u00e7\u00f5es feitas no projeto CONTRIBUTING.md - Guia de contribui\u00e7\u00e3o para o projeto CODE_OF_CONDUCT.md - C\u00f3digo de conduta para os colaboradores COLABORATORS.md - Lista de todos os colaboradores do projeto SECURITY.md - Informa\u00e7\u00f5es sobre como denunciar vulnerabilidades de seguran\u00e7a do projeto LICENSE - Licen\u00e7a do projeto Al\u00e9m disso, nos atentamos em criar padr\u00f5es para Issues e Pull Requests, para que a comunica\u00e7\u00e3o entre os colaboradores seja eficiente e clara. Os templates podem ser encontrados dentro de .github . Pull Request Bug Issue Feature Request Issue PULL_REQUEST_TEMPLATE.md bug-report.yml feature-request.yml Changelog Tentamos criar uma pipeline (localizada em .github/release.yml ) para automatizar a cria\u00e7\u00e3o de releases e a atualiza\u00e7\u00e3o do CHANGELOG.md . A ideia \u00e9 que a cada nova vers\u00e3o, o arquivo seja atualizado com as novas funcionalidades, corre\u00e7\u00f5es e melhorias feitas. Por\u00e9m n\u00e3o conseguimos criar uma release a tempo para testar o funcionamento. Deploy Para o deploy do MKDocs, utilizamos o GitHub Pages. A pipeline deploy-mkdocs.yml dentro de .github/workflows \u00e9 respons\u00e1vel por fazer o deploy autom\u00e1tico do site a cada nova altera\u00e7\u00e3o feita na branch main e verificando somente arquivos em que h\u00e1 altera\u00e7\u00f5es que geram uma nova vers\u00e3o do MKDocs. A ideia \u00e9 que o site seja atualizado automaticamente, sem a necessidade de interven\u00e7\u00e3o humana.","title":"Padr\u00f5es do Projeto"},{"location":"padroes/#padroes-do-projeto","text":"","title":"Padr\u00f5es do projeto"},{"location":"padroes/#documentacao","text":"Focamos em criar uma documenta\u00e7\u00e3o concisa, utilizando os melhores padr\u00f5es de escrita e organiza\u00e7\u00e3o de documentos. Para isso, utilizamos o Markdown, que \u00e9 uma linguagem de marca\u00e7\u00e3o leve e f\u00e1cil de usar, respons\u00e1vel tanto pelas documenta\u00e7\u00f5es que voc\u00ea est\u00e1 lendo e as que est\u00e3o dispon\u00edveis no reposit\u00f3rio. Os arquivos recomendados para a documenta\u00e7\u00e3o s\u00e3o: README.md - Documenta\u00e7\u00e3o principal do projeto CHANGELOG.md - Registro de todas as altera\u00e7\u00f5es feitas no projeto CONTRIBUTING.md - Guia de contribui\u00e7\u00e3o para o projeto CODE_OF_CONDUCT.md - C\u00f3digo de conduta para os colaboradores COLABORATORS.md - Lista de todos os colaboradores do projeto SECURITY.md - Informa\u00e7\u00f5es sobre como denunciar vulnerabilidades de seguran\u00e7a do projeto LICENSE - Licen\u00e7a do projeto Al\u00e9m disso, nos atentamos em criar padr\u00f5es para Issues e Pull Requests, para que a comunica\u00e7\u00e3o entre os colaboradores seja eficiente e clara. Os templates podem ser encontrados dentro de .github . Pull Request Bug Issue Feature Request Issue PULL_REQUEST_TEMPLATE.md bug-report.yml feature-request.yml","title":"Documenta\u00e7\u00e3o"},{"location":"padroes/#changelog","text":"Tentamos criar uma pipeline (localizada em .github/release.yml ) para automatizar a cria\u00e7\u00e3o de releases e a atualiza\u00e7\u00e3o do CHANGELOG.md . A ideia \u00e9 que a cada nova vers\u00e3o, o arquivo seja atualizado com as novas funcionalidades, corre\u00e7\u00f5es e melhorias feitas. Por\u00e9m n\u00e3o conseguimos criar uma release a tempo para testar o funcionamento.","title":"Changelog"},{"location":"padroes/#deploy","text":"Para o deploy do MKDocs, utilizamos o GitHub Pages. A pipeline deploy-mkdocs.yml dentro de .github/workflows \u00e9 respons\u00e1vel por fazer o deploy autom\u00e1tico do site a cada nova altera\u00e7\u00e3o feita na branch main e verificando somente arquivos em que h\u00e1 altera\u00e7\u00f5es que geram uma nova vers\u00e3o do MKDocs. A ideia \u00e9 que o site seja atualizado automaticamente, sem a necessidade de interven\u00e7\u00e3o humana.","title":"Deploy"}]}